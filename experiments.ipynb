{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GATLayerWithIRCRWR(nn.Module):\n",
    "    # These may change in the inductive setting - leaving it like this for now (not future proof)\n",
    "    nodes_dim = 0      # node dimension (axis is maybe a more familiar term nodes_dim is the position of \"N\" in tensor)\n",
    "    head_dim = 1       # attention head dim\n",
    "\n",
    "    def __init__(self, num_in_features, num_out_features, num_of_heads, gamma=0.1, beta=1, concat=True, activation=nn.ELU(),\n",
    "                 dropout_prob=0.6, random_walk_with_restart=True, add_residual_connection=True, bias=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_of_heads = num_of_heads\n",
    "        self.num_out_features = num_out_features\n",
    "        self.concat = concat  # whether we should concatenate or average the attention heads\n",
    "        self.residual_connection = add_residual_connection\n",
    "        self.random_walk_with_restart = random_walk_with_restart\n",
    "        self.gamma = gamma # RWR의 재시작 확률\n",
    "        self.beta = beta # residual connection의 가중치\n",
    "\n",
    "        #\n",
    "        # Trainable weights: linear projection matrix (denoted as \"W\" in the paper), attention target/source\n",
    "        # (denoted as \"a\" in the paper) and bias (not mentioned in the paper but present in the official GAT repo)\n",
    "        #\n",
    "\n",
    "        # You can treat this one matrix as num_of_heads independent W matrices\n",
    "        self.linear_proj = nn.Linear(num_in_features, num_of_heads * num_out_features, bias=False)\n",
    "\n",
    "        # After we concatenate target node (node i) and source node (node j) we apply the \"additive\" scoring function\n",
    "        # which gives us un-normalized score \"e\". Here we split the \"a\" vector - but the semantics remain the same.\n",
    "        # Basically instead of doing [x, y] (concatenation, x/y are node feature vectors) and dot product with \"a\"\n",
    "        # we instead do a dot product between x and \"a_left\" and y and \"a_right\" and we sum them up\n",
    "        # 즉 논문에서의 e_ij = LeakyReLU(a_T[ Wh_i ∣∣ Wh_j ])를 연산량을 줄이기 위해\n",
    "        # e_ij​ = LeakyReLU((Wh_i​)⋅a_left​ + (Wh_j)⋅a_right​)로 바꿔 수행한다.\n",
    "        self.scoring_fn_target = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n",
    "        self.scoring_fn_source = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n",
    "\n",
    "        # Bias is definitely not crucial to GAT - feel free to experiment (I pinged the main author, Petar, on this one)\n",
    "        if bias and concat:\n",
    "            self.bias = nn.Parameter(torch.Tensor(num_of_heads * num_out_features))\n",
    "        elif bias and not concat:\n",
    "            self.bias = nn.Parameter(torch.Tensor(num_out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        if self.residual_connection or self.random_walk_with_restart:\n",
    "            self.W_residual = nn.Linear(num_in_features, num_of_heads * num_out_features, bias=False)\n",
    "        else:\n",
    "            self.register_parameter('W_residual', None)\n",
    "\n",
    "        #\n",
    "        # End of trainable weights\n",
    "        #\n",
    "\n",
    "        self.leakyReLU = nn.LeakyReLU(0.2)  # using 0.2 as in the paper, no need to expose every setting\n",
    "        self.activation = activation\n",
    "        # Probably not the nicest design but I use the same module in 3 locations, before/after features projection\n",
    "        # and for attention coefficients. Functionality-wise it's the same as using independent modules.\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "        self.reset_parameter()\n",
    "        \n",
    "    def forward(self, data):\n",
    "        #\n",
    "        # Step 1: Linear Projection + regularization\n",
    "        #\n",
    "\n",
    "        in_nodes_features, edge_index, initial_features = data  # unpack data\n",
    "        if initial_features is None and (self.residual_connection or self.random_walk_with_restart):\n",
    "            initial_features = in_nodes_features\n",
    "            initial_features = self.W_residual(in_nodes_features).reshape(-1, self.num_of_heads, self.num_out_features)\n",
    "        num_of_nodes = in_nodes_features.shape[self.nodes_dim]\n",
    "        assert edge_index.shape[0] == 2, f'Expected edge index with shape=(2,E) got {edge_index.shape}'\n",
    "\n",
    "        # shape = (노드 수, 입력 특징 수) : 각 노드의 입력 특징을 나타낸다.\n",
    "        # 논문에서와 같이 모든 입력 노드 특징에 dropout을 적용한다.\n",
    "        # Note: for Cora features are already super sparse so it's questionable how much this actually helps\n",
    "        in_nodes_features = self.dropout(in_nodes_features) # 공식 GAT 구현에서도 dropout을 사용.\n",
    "\n",
    "        # shape = (노드 수, 입력 특징 수) * (입력 특징 수, 헤드 수 * 출력 특징 수) -> (노드 수, 헤드 수, 출력 특징 수)\n",
    "        # We project the input node features into NH independent output features (one for each attention head)\n",
    "        # 즉 각 노드의 특징을 각 헤드마다 다른 특징으로 변환한다.\n",
    "        nodes_features_proj = self.linear_proj(in_nodes_features).reshape(-1, self.num_of_heads, self.num_out_features)\n",
    "\n",
    "        nodes_features_proj = self.dropout(nodes_features_proj) # 공식 GAT 구현에서도 dropout을 사용.\n",
    "\n",
    "        #\n",
    "        # Step 2: Edge attention calculation\n",
    "        #\n",
    "\n",
    "        # Apply the scoring function (* represents element-wise (a.k.a. Hadamard) product)\n",
    "        # shape = (노드 수, 헤드 수, 출력 특징 수) * (1, 헤드 수, 출력 특징 수) -> (노드 수, 헤드 수, 1) -> (노드 수, 헤드 수)\n",
    "        # sum은 마지막 차원을 기준으로 sum하므로 (N, NH, FOUT) -> (N, NH) 즉, 노드 수 x 헤드 수\n",
    "        # 여기서 학습 가능한 파라미터인 scoring_fn_source, scoring_fn_target을 사용하여 각 노드의 특징을 계산한다.\n",
    "        # [GAT에선 a 벡터를 사용하여 두 노드의 특징을 결합한 후, scoring function을 통해 attention score를 계산한다.]\n",
    "        scores_source = (nodes_features_proj * self.scoring_fn_source).sum(dim=-1) # 마지막 차원을 기준으로 sum 즉, (N, NH, FOUT) -> (N, NH) 즉, 노드 수 x 헤드 수\n",
    "        scores_target = (nodes_features_proj * self.scoring_fn_target).sum(dim=-1) # (N, NH) 즉, 노드 수 x 헤드 수\n",
    "        \n",
    "        # We simply copy (lift) the scores for source/target nodes based on the edge index. Instead of preparing all\n",
    "        # the possible combinations of scores we just prepare those that will actually be used and those are defined\n",
    "        # by the edge index.\n",
    "        # scores shape = (E, NH), nodes_features_proj_lifted shape = (E, NH, FOUT), E - number of edges in the graph\n",
    "        # 각 엣지에 대한 source, target의 attention score를 계산한다.\n",
    "        scores_source_lifted, scores_target_lifted, nodes_features_proj_lifted = self.lift(scores_source, scores_target, nodes_features_proj, edge_index)\n",
    "        scores_per_edge = self.leakyReLU(scores_source_lifted + scores_target_lifted)\n",
    "\n",
    "        # shape = (엣지 수, 헤드 수, 1) -> (엣지 수, 헤드 수, 1) (unsqueeze를 통해 차원을 추가한다. 그래야 element-wise 곱을 할 수 있다.)\n",
    "        # 이제 softmax를 통해 attention coefficient를 계산한다.\n",
    "        attentions_per_edge = self.neighborhood_aware_softmax(scores_per_edge, edge_index[1], num_of_nodes)\n",
    "        # Add stochasticity to neighborhood aggregation\n",
    "        attentions_per_edge = self.dropout(attentions_per_edge)\n",
    "\n",
    "        #\n",
    "        # Step 3: Neighborhood aggregation\n",
    "        #\n",
    "\n",
    "        # Element-wise (aka Hadamard) product. Operator * does the same thing as torch.mul\n",
    "        # shape = (엣지 수, 헤드 수, 출력 특징 수) * (엣지 수, 헤드 수, 1) -> (엣지 수, 헤드 수, 출력 특징 수) 1이 FOUT으로 브로드캐스팅된다.\n",
    "        # FOUT은 출력 특징 수이다. 즉, 각 엣지의 attention score를 이용하여 각 노드의 특징을 가중합하여 계산한다.\n",
    "        nodes_features_proj_lifted_weighted = nodes_features_proj_lifted * attentions_per_edge\n",
    "\n",
    "        # 이 부분은 각 노드의 이웃 노드의 특징을 가중합하여 계산한다.\n",
    "        # shape = (노드 수, 헤드 수, 출력 특징 수)\n",
    "        out_nodes_features = self.aggregate_neighbors(initial_features, nodes_features_proj_lifted_weighted, edge_index, in_nodes_features, num_of_nodes)\n",
    "\n",
    "        #\n",
    "        # Step 4: Residual/skip connections, concat and bias\n",
    "        #\n",
    "\n",
    "        out_nodes_features = self.skip_concat_bias(out_nodes_features, initial_features)\n",
    "\n",
    "        return (out_nodes_features, edge_index, initial_features)\n",
    "    \n",
    "    def reset_parameter(self):\n",
    "        \"\"\"\n",
    "        원래 논문에서 GAT을 구현한 코드가 TensorFlow로 되어있고, 그 코드에서는 기본 초기화 방법으로 사용했기 때문에\n",
    "        Glorot (Xavier uniform) initialization을 사용한다.\n",
    "        \n",
    "        Tensorflow의 기본 초기화 방법은 Glorot (Xavier uniform) initialization이다.\n",
    "        https://stackoverflow.com/questions/37350131/what-is-the-default-variable-initializer-in-tensorflow\n",
    "\n",
    "        \"\"\"\n",
    "        nn.init.xavier_uniform_(self.linear_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.scoring_fn_target)\n",
    "        nn.init.xavier_uniform_(self.scoring_fn_source)\n",
    "        if self.residual_connection or self.random_walk_with_restart:\n",
    "            nn.init.xavier_uniform_(self.W_residual.weight)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            torch.nn.init.zeros_(self.bias)\n",
    "\n",
    "    def lift(self, scores_source, scores_target, nodes_features_matrix_proj, edge_index):\n",
    "        \"\"\"\n",
    "        Lifts i.e. duplicates certain vectors depending on the edge index.\n",
    "        One of the tensor dims goes from N -> E (that's where the \"lift\" comes from).\n",
    "        즉 lifts는 edge index에 따라 특정 벡터를 엣지 수만큼 복제한다.\n",
    "        텐서의 차원 중 하나가 N -> E로 변한다.\n",
    "        여기서 N은 노드 수, E는 엣지 수이다.\n",
    "\n",
    "        \"\"\"\n",
    "        # src_nodes_index :  tensor([   0,    0,    0,  ..., 2707, 2707, 2707])\n",
    "        # trg_nodes_index :  tensor([ 633, 1862, 2582,  ...,  598, 1473, 2706])\n",
    "        src_nodes_index = edge_index[0]\n",
    "        trg_nodes_index = edge_index[1]\n",
    "\n",
    "        # scores_source shape before :  torch.Size([2708, 8])\n",
    "        scores_source = scores_source.index_select(self.nodes_dim, src_nodes_index)\n",
    "        # scores_source shape after :  torch.Size([10556, 8])\n",
    "        scores_target = scores_target.index_select(self.nodes_dim, trg_nodes_index)\n",
    "        # nodes_features_proj shape torch.Size([2708, 8, 8])\n",
    "        nodes_features_matrix_proj_lifted = nodes_features_matrix_proj.index_select(self.nodes_dim, src_nodes_index)\n",
    "        # nodes_features_matrix_proj_lifted shape :  torch.Size([10556, 8, 8])\n",
    "\n",
    "        return scores_source, scores_target, nodes_features_matrix_proj_lifted\n",
    "\n",
    "    def neighborhood_aware_softmax(self, scores_per_edge, trg_index, num_of_nodes):\n",
    "        \"\"\"\n",
    "        이웃 노드들의 attention score를 softmax를 통해 계산한다.\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate the numerator. Make logits <= 0 so that e^logit <= 1 (this will improve the numerical stability)\n",
    "        # https://stats.stackexchange.com/questions/338285/how-does-the-subtraction-of-the-logit-maximum-improve-learning\n",
    "        scores_per_edge = scores_per_edge - scores_per_edge.max()\n",
    "        exp_scores_per_edge = scores_per_edge.exp()\n",
    "\n",
    "        # Calculate the denominator. shape = (E, NH)\n",
    "        neigborhood_aware_denominator = self.sum_edge_scores_neighborhood_aware(exp_scores_per_edge, trg_index, num_of_nodes)\n",
    "\n",
    "        # 1e-16은 이론적으로 필요하지 않지만 수치적 안정성을 위해 (0으로 나누는 것을 피하기 위해) 추가했다.\n",
    "        attentions_per_edge = exp_scores_per_edge / (neigborhood_aware_denominator + 1e-16)\n",
    "        # shape = (E, NH) -> (E, NH, 1)로 만들어서 projected node features와 element-wise 곱을 할 수 있게 한다.\n",
    "\n",
    "        return attentions_per_edge.unsqueeze(-1)\n",
    "    \n",
    "    def sum_edge_scores_neighborhood_aware(self, exp_scores_per_edge, trg_index, num_of_nodes):\n",
    "        # attention head 개수만큼 브로드캐스팅한다. 여기서 브로드캐스팅이란 차원을 늘려서 연산을 수행하는 것을 의미한다.\n",
    "        # E -> (E, NH)\n",
    "        trg_index_broadcasted = self.explicit_broadcast(trg_index, exp_scores_per_edge)\n",
    "\n",
    "        # shape = (N, NH)\n",
    "        size = list(exp_scores_per_edge.shape)  # convert to list otherwise assignment is not possible\n",
    "        size[self.nodes_dim] = num_of_nodes\n",
    "        neighborhood_sums = torch.zeros(size, dtype=exp_scores_per_edge.dtype, device=exp_scores_per_edge.device)\n",
    "\n",
    "        # exp_scores_per_edge를 trg_index_broadcasted의 값을 index로 사용하여 neighborhood_sums에 더한다.\n",
    "        # 그렇게 되면 각 노드의 이웃 노드들의 attention score의 합을 계산할 수 있다.\n",
    "        neighborhood_sums.scatter_add_(self.nodes_dim, trg_index_broadcasted, exp_scores_per_edge)\n",
    "\n",
    "        # edge_index의 차원에 맞게 브로드캐스팅한다.\n",
    "        # 모든 location의 값이 i번째 노드의 attention score의 합으로 브로드캐스팅되는 것이다.\n",
    "        # shape = (N, NH) -> (E, NH)\n",
    "        return neighborhood_sums.index_select(self.nodes_dim, trg_index)\n",
    "\n",
    "    def aggregate_neighbors(self, initial_features, nodes_features_proj_lifted_weighted, edge_index, in_nodes_features, num_of_nodes):\n",
    "        # shape = (E, NH, FOUT) -> (N, NH, FOUT)\n",
    "        out_nodes_features = torch.zeros(num_of_nodes, *nodes_features_proj_lifted_weighted.shape[1:], dtype=in_nodes_features.dtype, device=in_nodes_features.device)\n",
    "\n",
    "        # shape = (E) -> (E, NH, FOUT)\n",
    "        trg_index_broadcasted = self.explicit_broadcast(edge_index[1], nodes_features_proj_lifted_weighted)\n",
    "        # aggregation step - we accumulate projected, weighted node features for all the attention heads\n",
    "        # shape = (E, NH, FOUT) -> (N, NH, FOUT)\n",
    "        out_nodes_features.scatter_add_(self.nodes_dim, trg_index_broadcasted, nodes_features_proj_lifted_weighted)\n",
    "\n",
    "        # Random walk with Restart\n",
    "        if self.random_walk_with_restart:\n",
    "            out_nodes_features = (1 - self.gamma) * out_nodes_features + self.gamma * initial_features\n",
    "\n",
    "        return out_nodes_features\n",
    "\n",
    "    def explicit_broadcast(self, this, other):\n",
    "        # 차원이 같아질 때까지 singleton 차원을 추가한다.\n",
    "        for _ in range(this.dim(), other.dim()):\n",
    "            this = this.unsqueeze(-1) # 가장 마지막 차원을 추가한다.\n",
    "\n",
    "        # other 텐서와 같은 모양으로 확장하는데, 이 때 실제로 데이터를 복사하지는 않고, 필요에 따라 가상적으로 차원을 확장한다.\n",
    "        return this.expand_as(other)\n",
    "    \n",
    "    def skip_concat_bias(self, out_nodes_features, initial_features):\n",
    "        if self.residual_connection:  # add residual connection\n",
    "            if out_nodes_features.shape[-1] == initial_features.shape[-1]:  # if FIN == FOUT\n",
    "                out_nodes_features += initial_features\n",
    "        if self.concat:\n",
    "            # shape = (N, NH, FOUT) -> (N, NH*FOUT)\n",
    "            out_nodes_features = out_nodes_features.reshape(-1, self.num_of_heads * self.num_out_features)\n",
    "        else:\n",
    "            # shape = (N, NH, FOUT) -> (N, FOUT)\n",
    "            out_nodes_features = out_nodes_features.mean(dim=self.head_dim)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out_nodes_features += self.bias\n",
    "\n",
    "        return out_nodes_features if self.activation is None else self.activation(out_nodes_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reference: GRAPH ATTENTION NETWORKS (2018).\n",
    "\n",
    "https://github.com/PetarV-/GAT\n",
    "https://github.com/gordicaleksa/pytorch-GAT\n",
    "\"\"\"\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class GATWithIRCRWR(nn.Module):\n",
    "    def __init__(self, num_of_additional_layer, num_in_features, num_classes, random_walk_with_restart=True, add_residual_connection=True, bias=True, dropout=0.6):\n",
    "        super().__init__()\n",
    "\n",
    "        additional_layers = []\n",
    "\n",
    "        for _ in range(num_of_additional_layer):\n",
    "            additional_layers.append(\n",
    "                    GATLayerWithIRCRWR(\n",
    "                    num_in_features=8*8,  # consequence of concatenation\n",
    "                    num_out_features=8,\n",
    "                    num_of_heads=8,\n",
    "                    concat=True,\n",
    "                    activation=nn.ELU(),\n",
    "                    dropout_prob=dropout,\n",
    "                    random_walk_with_restart=random_walk_with_restart,\n",
    "                    add_residual_connection=add_residual_connection,\n",
    "                    bias=bias\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        self.gat_net = nn.Sequential(\n",
    "            GATLayerWithIRCRWR(\n",
    "                num_in_features=num_in_features,  # consequence of concatenation\n",
    "                num_out_features=8,\n",
    "                num_of_heads=8,\n",
    "                concat=True,\n",
    "                activation=nn.ELU(),\n",
    "                dropout_prob=dropout,\n",
    "                random_walk_with_restart=random_walk_with_restart,\n",
    "                add_residual_connection=add_residual_connection,\n",
    "                bias=bias\n",
    "            ),\n",
    "            *additional_layers,\n",
    "            GATLayerWithIRCRWR(\n",
    "                num_in_features=8 * 8,  # consequence of concatenation\n",
    "                num_out_features=num_classes,\n",
    "                num_of_heads=1,\n",
    "                concat=False,  # last GAT layer does mean avg, the others do concat\n",
    "                activation=None,  # last layer just outputs raw scores\n",
    "                dropout_prob=dropout,\n",
    "                random_walk_with_restart=False,\n",
    "                add_residual_connection=False,\n",
    "                bias=bias\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # data is just a (in_nodes_features, edge_index) tuple, I had to do it like this because of the nn.Sequential:\n",
    "    # https://discuss.pytorch.org/t/forward-takes-2-positional-arguments-but-3-were-given-for-nn-sqeuential-with-linear-layers/65698\n",
    "    def forward(self, data):\n",
    "        data = data + (None,)\n",
    "        return self.gat_net(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "# 3 different model training/eval phases used in train.py\n",
    "class LoopPhase(enum.Enum):\n",
    "    TRAIN = 0,\n",
    "    VAL = 1,\n",
    "    TEST = 2\n",
    "\n",
    "# Global vars used for early stopping. After some number of epochs (as defined by the patience_period var) without any\n",
    "# improvement on the validation dataset (measured via accuracy metric), we'll break out from the training loop.\n",
    "BEST_VAL_ACC = 0\n",
    "BEST_VAL_LOSS = 0\n",
    "\n",
    "\n",
    "def get_training_args(time_start, dataset, train_range, val_range, test_range, num_input_features, num_classes, random_walk_with_restart, add_residual_connection, num_of_additional_layer=0):\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Training related\n",
    "    parser.add_argument(\"--num_of_epochs\", type=int, help=\"number of training epochs\", default=10000)\n",
    "    parser.add_argument(\"--patience_period\", type=int, help=\"number of epochs with no improvement on val before terminating\", default=1000)\n",
    "    parser.add_argument(\"--lr\", type=float, help=\"model learning rate\", default=5e-3)\n",
    "    parser.add_argument(\"--weight_decay\", type=float, help=\"L2 regularization on model weights\", default=5e-4)\n",
    "    parser.add_argument(\"--should_test\", type=bool, help='should test the model on the test dataset?', default=True)\n",
    "\n",
    "    # Dataset related\n",
    "    # parser.add_argument(\"--dataset_name\", choices=[el.name for el in DatasetType], help='dataset to use for training', default=DatasetType.CORA.name)\n",
    "    parser.add_argument(\"--should_visualize\", type=bool, help='should visualize the dataset?', default=False)\n",
    "\n",
    "    # Logging/debugging/checkpoint related (helps a lot with experimentation)\n",
    "    parser.add_argument(\"--enable_tensorboard\", type=bool, help=\"enable tensorboard logging\", default=False)\n",
    "    parser.add_argument(\"--console_log_freq\", type=int, help=\"log to output console (epoch) freq (None for no logging)\", default=100)\n",
    "    parser.add_argument(\"--checkpoint_freq\", type=int, help=\"checkpoint model saving (epoch) freq (None for no logging)\", default=1000)\n",
    "    args = parser.parse_args(\"\")\n",
    "\n",
    "    # Model architecture related - this is the architecture as defined in the official paper (for Cora classification)\n",
    "    gat_config = {\n",
    "        \"num_of_layers\": 2,  # GNNs, contrary to CNNs, are often shallow (it ultimately depends on the graph properties)\n",
    "        \"num_heads_per_layer\": [8, 1],\n",
    "        \"num_features_per_layer\": [num_input_features, 8, num_classes],\n",
    "        \"add_skip_connection\": False,  # hurts perf on Cora\n",
    "        \"bias\": True,  # result is not so sensitive to bias\n",
    "        \"dropout\": 0.6,  # result is sensitive to dropout\n",
    "        \"dataset\": dataset,\n",
    "        \"time_start\": time_start,\n",
    "        \"train_range\": train_range,\n",
    "        \"val_range\": val_range,\n",
    "        \"test_range\": test_range,\n",
    "        \"random_walk_with_restart\": random_walk_with_restart,\n",
    "        \"add_residual_connection\": add_residual_connection,\n",
    "        \"num_of_additional_layer\": num_of_additional_layer\n",
    "    }\n",
    "\n",
    "    # Wrapping training configuration into a dictionary\n",
    "    training_config = dict()\n",
    "    for arg in vars(args):\n",
    "        training_config[arg] = getattr(args, arg)\n",
    "\n",
    "    # Add additional config information\n",
    "    training_config.update(gat_config)\n",
    "\n",
    "    return training_config\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "def train_gat(config, save_to_file, filename):\n",
    "    global BEST_VAL_ACC, BEST_VAL_LOSS\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # checking whether you have a GPU, I hope so!\n",
    "\n",
    "    # Step 1: load the graph data\n",
    "    # node_features, node_labels, edge_index, train_indices, val_indices, test_indices = load_graph_data(config, device)\n",
    "    # Step 1: load the graph data\n",
    "    node_features = config[\"dataset\"][0].x.to(device)\n",
    "    node_labels = config[\"dataset\"][0].y.to(device)\n",
    "    edge_index = config[\"dataset\"][0].edge_index.to(device)\n",
    "\n",
    "    # Indices that help us extract nodes that belong to the train/val and test splits\n",
    "    train_indices = torch.arange(config[\"train_range\"][0], config[\"train_range\"][1], dtype=torch.long, device=device)\n",
    "    val_indices = torch.arange(config[\"val_range\"][0], config[\"val_range\"][1], dtype=torch.long, device=device)\n",
    "    test_indices = torch.arange(config[\"test_range\"][0], config[\"test_range\"][1], dtype=torch.long, device=device)\n",
    "\n",
    "    # Step 2: prepare the model\n",
    "    gat = GATWithIRCRWR(config[\"num_of_additional_layer\"], config[\"num_features_per_layer\"][0], config[\"num_features_per_layer\"][-1], random_walk_with_restart=config[\"random_walk_with_restart\"], add_residual_connection=config[\"add_residual_connection\"], bias=True, dropout=0.6).to(device)\n",
    "    # gat = GAT(1433, 7, add_skip_connection=False, bias=True, dropout=0.6).to(device)\n",
    "\n",
    "    # Step 3: Prepare other training related utilities (loss & optimizer and decorator function)\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "    optimizer = Adam(gat.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "\n",
    "    # THIS IS THE CORE OF THE TRAINING (we'll define it in a minute)\n",
    "    # The decorator function makes things cleaner since there is a lot of redundancy between the train and val loops\n",
    "    main_loop = get_main_loop(\n",
    "        config,\n",
    "        gat,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        node_features,\n",
    "        node_labels,\n",
    "        edge_index,\n",
    "        train_indices,\n",
    "        val_indices,\n",
    "        test_indices,\n",
    "        config['patience_period'],\n",
    "        time.time())\n",
    "\n",
    "    BEST_VAL_ACC, BEST_VAL_LOSS, PATIENCE_CNT = [0, 0, 0]  # reset vars used for early stopping\n",
    "\n",
    "    # Step 4: Start the training procedure\n",
    "    for epoch in range(config['num_of_epochs']):\n",
    "        # Training loop\n",
    "        main_loop(phase=LoopPhase.TRAIN, epoch=epoch, save_to_file=save_to_file, filename=filename)\n",
    "\n",
    "        # Validation loop\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                main_loop(phase=LoopPhase.VAL, epoch=epoch, save_to_file=save_to_file, filename=filename)\n",
    "            except Exception as e:  # \"patience has run out\" exception :O\n",
    "                print(str(e))\n",
    "                break  # break out from the training loop\n",
    "\n",
    "    # Step 5: Potentially test your model\n",
    "    # Don't overfit to the test dataset - only when you've fine-tuned your model on the validation dataset should you\n",
    "    # report your final loss and accuracy on the test dataset. Friends don't let friends overfit to the test data. <3\n",
    "    if config['should_test']:\n",
    "        test_acc = main_loop(phase=LoopPhase.TEST)\n",
    "        config['test_acc'] = test_acc\n",
    "        save_to_file(filename, f'Test accuracy = {test_acc}\\n')\n",
    "        print(f'Test accuracy = {test_acc}')\n",
    "    else:\n",
    "        config['test_acc'] = -1\n",
    "\n",
    "    # Save the latest GAT in the binaries directory\n",
    "    # torch.save(get_training_state(config, gat), os.path.join(BINARIES_PATH, get_available_binary_name()))\n",
    "\n",
    "# Simple decorator function so that I don't have to pass arguments that don't change from epoch to epoch\n",
    "def get_main_loop(config, gat, cross_entropy_loss, optimizer, node_features, node_labels, edge_index, train_indices, val_indices, test_indices, patience_period, time_start):\n",
    "\n",
    "    node_dim = 0  # this will likely change as soon as I add an inductive example (Cora is transductive)\n",
    "\n",
    "    train_labels = node_labels.index_select(node_dim, train_indices)\n",
    "    val_labels = node_labels.index_select(node_dim, val_indices)\n",
    "    test_labels = node_labels.index_select(node_dim, test_indices)\n",
    "\n",
    "    # node_features shape = (N, FIN), edge_index shape = (2, E)\n",
    "    graph_data = (node_features, edge_index)  # I pack data into tuples because GAT uses nn.Sequential which requires it\n",
    "\n",
    "    def get_node_indices(phase):\n",
    "        if phase == LoopPhase.TRAIN:\n",
    "            return train_indices\n",
    "        elif phase == LoopPhase.VAL:\n",
    "            return val_indices\n",
    "        else:\n",
    "            return test_indices\n",
    "\n",
    "    def get_node_labels(phase):\n",
    "        if phase == LoopPhase.TRAIN:\n",
    "            return train_labels\n",
    "        elif phase == LoopPhase.VAL:\n",
    "            return val_labels\n",
    "        else:\n",
    "            return test_labels\n",
    "\n",
    "    def main_loop(phase, epoch=0, save_to_file=None, filename=None):\n",
    "        global BEST_VAL_ACC, BEST_VAL_LOSS, PATIENCE_CNT, writer\n",
    "\n",
    "        # Certain modules behave differently depending on whether we're training the model or not.\n",
    "        # e.g. nn.Dropout - we only want to drop model weights during the training.\n",
    "        if phase == LoopPhase.TRAIN:\n",
    "            gat.train()\n",
    "        else:\n",
    "            gat.eval()\n",
    "\n",
    "        node_indices = get_node_indices(phase)\n",
    "        gt_node_labels = get_node_labels(phase)  # gt stands for ground truth\n",
    "\n",
    "        # Do a forwards pass and extract only the relevant node scores (train/val or test ones)\n",
    "        # Note: [0] just extracts the node_features part of the data (index 1 contains the edge_index)\n",
    "        # shape = (N, C) where N is the number of nodes in the split (train/val/test) and C is the number of classes\n",
    "        nodes_unnormalized_scores = gat(graph_data)[0].index_select(node_dim, node_indices)\n",
    "\n",
    "        # Example: let's take an output for a single node on Cora - it's a vector of size 7 and it contains unnormalized\n",
    "        # scores like: V = [-1.393,  3.0765, -2.4445,  9.6219,  2.1658, -5.5243, -4.6247]\n",
    "        # What PyTorch's cross entropy loss does is for every such vector it first applies a softmax, and so we'll\n",
    "        # have the V transformed into: [1.6421e-05, 1.4338e-03, 5.7378e-06, 0.99797, 5.7673e-04, 2.6376e-07, 6.4848e-07]\n",
    "        # secondly, whatever the correct class is (say it's 3), it will then take the element at position 3,\n",
    "        # 0.99797 in this case, and the loss will be -log(0.99797). It does this for every node and applies a mean.\n",
    "        # You can see that as the probability of the correct class for most nodes approaches 1 we get to 0 loss! <3\n",
    "        loss = cross_entropy_loss(nodes_unnormalized_scores, gt_node_labels)\n",
    "\n",
    "        if phase == LoopPhase.TRAIN:\n",
    "            optimizer.zero_grad()  # clean the trainable weights gradients in the computational graph (.grad fields)\n",
    "            loss.backward()  # compute the gradients for every trainable weight in the computational graph\n",
    "            optimizer.step()  # apply the gradients to weights\n",
    "\n",
    "        # Finds the index of maximum (unnormalized) score for every node and that's the class prediction for that node.\n",
    "        # Compare those to true (ground truth) labels and find the fraction of correct predictions -> accuracy metric.\n",
    "        class_predictions = torch.argmax(nodes_unnormalized_scores, dim=-1)\n",
    "        accuracy = torch.sum(torch.eq(class_predictions, gt_node_labels).long()).item() / len(gt_node_labels)\n",
    "\n",
    "        #\n",
    "        # Logging\n",
    "        #\n",
    "\n",
    "        if phase == LoopPhase.TRAIN:\n",
    "            # Log metrics\n",
    "            if config['enable_tensorboard']:\n",
    "                writer.add_scalar('training_loss', loss.item(), epoch)\n",
    "                writer.add_scalar('training_acc', accuracy, epoch)\n",
    "\n",
    "            # Save model checkpoint\n",
    "            if config['checkpoint_freq'] is not None and (epoch + 1) % config['checkpoint_freq'] == 0:\n",
    "                ckpt_model_name = f\"gat_ckpt_epoch_{epoch + 1}.pth\"\n",
    "                config['test_acc'] = -1\n",
    "                # torch.save(get_training_state(config, gat), os.path.join(CHECKPOINTS_PATH, ckpt_model_name))\n",
    "\n",
    "        elif phase == LoopPhase.VAL:\n",
    "            # Log metrics\n",
    "            if config['enable_tensorboard']:\n",
    "                writer.add_scalar('val_loss', loss.item(), epoch)\n",
    "                writer.add_scalar('val_acc', accuracy, epoch)\n",
    "\n",
    "            # Log to console\n",
    "            if config['console_log_freq'] is not None and epoch % config['console_log_freq'] == 0:\n",
    "                save_to_file(filename, f'GAT training: time elapsed= {(time.time() - time_start):.2f} [s] | epoch={epoch + 1} | val acc={accuracy}\\n')\n",
    "                print(f'GAT training: time elapsed= {(time.time() - time_start):.2f} [s] | epoch={epoch + 1} | val acc={accuracy}')\n",
    "\n",
    "            # The \"patience\" logic - should we break out from the training loop? If either validation acc keeps going up\n",
    "            # or the val loss keeps going down we won't stop\n",
    "            if accuracy > BEST_VAL_ACC or loss.item() < BEST_VAL_LOSS:\n",
    "                BEST_VAL_ACC = max(accuracy, BEST_VAL_ACC)  # keep track of the best validation accuracy so far\n",
    "                BEST_VAL_LOSS = min(loss.item(), BEST_VAL_LOSS)\n",
    "                PATIENCE_CNT = 0  # reset the counter every time we encounter new best accuracy\n",
    "            else:\n",
    "                PATIENCE_CNT += 1  # otherwise keep counting\n",
    "\n",
    "            if PATIENCE_CNT >= patience_period:\n",
    "                raise Exception('Stopping the training, the universe has no more patience for this training.')\n",
    "\n",
    "        else:\n",
    "            return accuracy  # in the case of test phase we just report back the test accuracy\n",
    "\n",
    "    return main_loop  # return the decorated function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cora\n",
      "2  layers  GAT\n",
      "GAT training: time elapsed= 0.01 [s] | epoch=1 | val acc=0.29\n",
      "GAT training: time elapsed= 0.90 [s] | epoch=101 | val acc=0.802\n",
      "GAT training: time elapsed= 1.81 [s] | epoch=201 | val acc=0.776\n",
      "GAT training: time elapsed= 2.72 [s] | epoch=301 | val acc=0.8\n",
      "GAT training: time elapsed= 3.63 [s] | epoch=401 | val acc=0.786\n",
      "GAT training: time elapsed= 4.56 [s] | epoch=501 | val acc=0.782\n",
      "GAT training: time elapsed= 5.46 [s] | epoch=601 | val acc=0.774\n",
      "GAT training: time elapsed= 6.34 [s] | epoch=701 | val acc=0.788\n",
      "GAT training: time elapsed= 7.23 [s] | epoch=801 | val acc=0.786\n",
      "GAT training: time elapsed= 8.14 [s] | epoch=901 | val acc=0.796\n",
      "GAT training: time elapsed= 9.04 [s] | epoch=1001 | val acc=0.784\n",
      "GAT training: time elapsed= 9.95 [s] | epoch=1101 | val acc=0.786\n",
      "Stopping the training, the universe has no more patience for this training.\n",
      "Test accuracy = 0.793\n",
      "Total training time: 9.99 [s]\n",
      "2  layers  GAT with Random walk with restart\n",
      "GAT training: time elapsed= 0.01 [s] | epoch=1 | val acc=0.174\n",
      "GAT training: time elapsed= 0.90 [s] | epoch=101 | val acc=0.77\n",
      "GAT training: time elapsed= 1.80 [s] | epoch=201 | val acc=0.786\n",
      "GAT training: time elapsed= 2.71 [s] | epoch=301 | val acc=0.788\n",
      "GAT training: time elapsed= 3.64 [s] | epoch=401 | val acc=0.78\n",
      "GAT training: time elapsed= 4.54 [s] | epoch=501 | val acc=0.798\n",
      "GAT training: time elapsed= 5.45 [s] | epoch=601 | val acc=0.778\n",
      "GAT training: time elapsed= 6.41 [s] | epoch=701 | val acc=0.784\n",
      "GAT training: time elapsed= 7.38 [s] | epoch=801 | val acc=0.782\n",
      "GAT training: time elapsed= 8.31 [s] | epoch=901 | val acc=0.78\n",
      "GAT training: time elapsed= 9.23 [s] | epoch=1001 | val acc=0.776\n",
      "GAT training: time elapsed= 10.16 [s] | epoch=1101 | val acc=0.782\n",
      "GAT training: time elapsed= 11.11 [s] | epoch=1201 | val acc=0.78\n",
      "GAT training: time elapsed= 12.00 [s] | epoch=1301 | val acc=0.78\n",
      "GAT training: time elapsed= 12.94 [s] | epoch=1401 | val acc=0.778\n",
      "GAT training: time elapsed= 13.91 [s] | epoch=1501 | val acc=0.786\n",
      "Stopping the training, the universe has no more patience for this training.\n",
      "Test accuracy = 0.795\n",
      "Total training time: 13.95 [s]\n",
      "2  layers  GAT with Initial residual connection\n",
      "GAT training: time elapsed= 0.01 [s] | epoch=1 | val acc=0.232\n",
      "GAT training: time elapsed= 0.93 [s] | epoch=101 | val acc=0.764\n",
      "GAT training: time elapsed= 1.86 [s] | epoch=201 | val acc=0.768\n",
      "GAT training: time elapsed= 2.80 [s] | epoch=301 | val acc=0.766\n",
      "GAT training: time elapsed= 3.71 [s] | epoch=401 | val acc=0.766\n",
      "GAT training: time elapsed= 4.62 [s] | epoch=501 | val acc=0.768\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m layers \u001b[39m\u001b[38;5;124m\"\u001b[39m, case[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     22\u001b[0m time_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 23\u001b[0m \u001b[43mtrain_gat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_training_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCORA_TRAIN_RANGE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCORA_VAL_RANGE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCORA_TEST_RANGE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCORA_NUM_INPUT_FEATURES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCORA_NUM_CLASSES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_walk_with_restart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcase\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_residual_connection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcase\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_of_additional_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# train_gat(time_start, dataset, CORA_TRAIN_RANGE, CORA_VAL_RANGE, CORA_TEST_RANGE, CORA_NUM_INPUT_FEATURES, CORA_NUM_CLASSES)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal training time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mtime_start)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m [s]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[54], line 117\u001b[0m, in \u001b[0;36mtrain_gat\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Step 4: Start the training procedure\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_of_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[43mmain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLoopPhase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;66;03m# Validation loop\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "Cell \u001b[0;32mIn[54], line 197\u001b[0m, in \u001b[0;36mget_main_loop.<locals>.main_loop\u001b[0;34m(phase, epoch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m phase \u001b[38;5;241m==\u001b[39m LoopPhase\u001b[38;5;241m.\u001b[39mTRAIN:\n\u001b[1;32m    196\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# clean the trainable weights gradients in the computational graph (.grad fields)\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# compute the gradients for every trainable weight in the computational graph\u001b[39;00m\n\u001b[1;32m    198\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# apply the gradients to weights\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# Finds the index of maximum (unnormalized) score for every node and that's the class prediction for that node.\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# Compare those to true (ground truth) labels and find the fraction of correct predictions -> accuracy metric.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "#\n",
    "# Cora specific constants\n",
    "#\n",
    "\n",
    "# Thomas Kipf et al. first used this split in GCN paper and later Petar Veličković et al. in GAT paper\n",
    "CORA_TRAIN_RANGE = [0, 140]  # we're using the first 140 nodes as the training nodes\n",
    "CORA_VAL_RANGE = [140, 140+500]\n",
    "CORA_TEST_RANGE = [1708, 1708+1000]\n",
    "CORA_NUM_INPUT_FEATURES = 1433\n",
    "CORA_NUM_CLASSES = 7\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print('Cora')\n",
    "    dataset = Planetoid(root='./data/Cora', name='Cora')\n",
    "    for i in range(4):\n",
    "        cases = [[False, False, \"GAT\"], [True, False, \"GAT with Random walk with restart\"], [False, True, \"GAT with Initial residual connection\"], [True, True, \"GAT with Random walk with restart and Initial residual connection\"]]\n",
    "        for case in cases:\n",
    "            print(i+2, \" layers \", case[-1])\n",
    "            time_start = time.time()\n",
    "            train_gat(get_training_args(time_start, dataset, CORA_TRAIN_RANGE, CORA_VAL_RANGE, CORA_TEST_RANGE, CORA_NUM_INPUT_FEATURES, CORA_NUM_CLASSES, random_walk_with_restart=case[0], add_residual_connection=case[1], num_of_additional_layer=i))\n",
    "            # train_gat(time_start, dataset, CORA_TRAIN_RANGE, CORA_VAL_RANGE, CORA_TEST_RANGE, CORA_NUM_INPUT_FEATURES, CORA_NUM_CLASSES)\n",
    "            print(f'Total training time: {(time.time() - time_start):.2f} [s]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT training: time elapsed= 0.01 [s] | epoch=1 | val acc=0.132\n",
      "GAT training: time elapsed= 0.93 [s] | epoch=101 | val acc=0.784\n",
      "GAT training: time elapsed= 1.90 [s] | epoch=201 | val acc=0.782\n",
      "GAT training: time elapsed= 2.76 [s] | epoch=301 | val acc=0.796\n",
      "GAT training: time elapsed= 3.72 [s] | epoch=401 | val acc=0.778\n",
      "GAT training: time elapsed= 4.60 [s] | epoch=501 | val acc=0.784\n",
      "GAT training: time elapsed= 5.54 [s] | epoch=601 | val acc=0.782\n",
      "GAT training: time elapsed= 6.42 [s] | epoch=701 | val acc=0.784\n",
      "GAT training: time elapsed= 7.33 [s] | epoch=801 | val acc=0.778\n",
      "GAT training: time elapsed= 8.24 [s] | epoch=901 | val acc=0.794\n",
      "GAT training: time elapsed= 9.20 [s] | epoch=1001 | val acc=0.782\n",
      "GAT training: time elapsed= 10.14 [s] | epoch=1101 | val acc=0.79\n",
      "GAT training: time elapsed= 11.05 [s] | epoch=1201 | val acc=0.774\n",
      "GAT training: time elapsed= 11.98 [s] | epoch=1301 | val acc=0.774\n",
      "GAT training: time elapsed= 12.86 [s] | epoch=1401 | val acc=0.776\n",
      "GAT training: time elapsed= 13.79 [s] | epoch=1501 | val acc=0.786\n",
      "GAT training: time elapsed= 14.64 [s] | epoch=1601 | val acc=0.792\n",
      "GAT training: time elapsed= 15.58 [s] | epoch=1701 | val acc=0.788\n",
      "GAT training: time elapsed= 16.45 [s] | epoch=1801 | val acc=0.788\n",
      "GAT training: time elapsed= 17.35 [s] | epoch=1901 | val acc=0.796\n",
      "Stopping the training, the universe has no more patience for this training.\n",
      "Test accuracy = 0.792\n",
      "GAT training: time elapsed= 0.08 [s] | epoch=1 | val acc=0.206\n",
      "GAT training: time elapsed= 1.06 [s] | epoch=101 | val acc=0.808\n",
      "GAT training: time elapsed= 2.01 [s] | epoch=201 | val acc=0.802\n",
      "GAT training: time elapsed= 3.00 [s] | epoch=301 | val acc=0.788\n",
      "GAT training: time elapsed= 3.97 [s] | epoch=401 | val acc=0.772\n",
      "GAT training: time elapsed= 4.90 [s] | epoch=501 | val acc=0.778\n",
      "GAT training: time elapsed= 5.87 [s] | epoch=601 | val acc=0.77\n",
      "GAT training: time elapsed= 6.84 [s] | epoch=701 | val acc=0.782\n",
      "GAT training: time elapsed= 7.79 [s] | epoch=801 | val acc=0.784\n",
      "GAT training: time elapsed= 8.74 [s] | epoch=901 | val acc=0.784\n",
      "GAT training: time elapsed= 9.65 [s] | epoch=1001 | val acc=0.796\n",
      "Stopping the training, the universe has no more patience for this training.\n",
      "Test accuracy = 0.798\n",
      "GAT training: time elapsed= 0.01 [s] | epoch=1 | val acc=0.228\n",
      "GAT training: time elapsed= 0.96 [s] | epoch=101 | val acc=0.774\n",
      "GAT training: time elapsed= 1.90 [s] | epoch=201 | val acc=0.758\n",
      "GAT training: time elapsed= 2.84 [s] | epoch=301 | val acc=0.766\n",
      "GAT training: time elapsed= 3.81 [s] | epoch=401 | val acc=0.758\n",
      "GAT training: time elapsed= 4.76 [s] | epoch=501 | val acc=0.762\n",
      "GAT training: time elapsed= 5.68 [s] | epoch=601 | val acc=0.756\n",
      "GAT training: time elapsed= 6.63 [s] | epoch=701 | val acc=0.75\n",
      "GAT training: time elapsed= 7.58 [s] | epoch=801 | val acc=0.76\n",
      "GAT training: time elapsed= 8.48 [s] | epoch=901 | val acc=0.762\n",
      "GAT training: time elapsed= 9.36 [s] | epoch=1001 | val acc=0.76\n",
      "GAT training: time elapsed= 10.31 [s] | epoch=1101 | val acc=0.76\n",
      "Stopping the training, the universe has no more patience for this training.\n",
      "Test accuracy = 0.79\n",
      "GAT training: time elapsed= 0.01 [s] | epoch=1 | val acc=0.278\n",
      "GAT training: time elapsed= 0.99 [s] | epoch=101 | val acc=0.734\n",
      "GAT training: time elapsed= 1.99 [s] | epoch=201 | val acc=0.758\n",
      "GAT training: time elapsed= 2.99 [s] | epoch=301 | val acc=0.764\n",
      "GAT training: time elapsed= 3.95 [s] | epoch=401 | val acc=0.744\n",
      "GAT training: time elapsed= 4.90 [s] | epoch=501 | val acc=0.738\n",
      "GAT training: time elapsed= 5.91 [s] | epoch=601 | val acc=0.742\n",
      "GAT training: time elapsed= 6.91 [s] | epoch=701 | val acc=0.766\n",
      "GAT training: time elapsed= 7.85 [s] | epoch=801 | val acc=0.758\n",
      "GAT training: time elapsed= 8.87 [s] | epoch=901 | val acc=0.758\n",
      "GAT training: time elapsed= 9.82 [s] | epoch=1001 | val acc=0.758\n",
      "GAT training: time elapsed= 10.79 [s] | epoch=1101 | val acc=0.752\n",
      "GAT training: time elapsed= 11.74 [s] | epoch=1201 | val acc=0.766\n",
      "GAT training: time elapsed= 12.73 [s] | epoch=1301 | val acc=0.764\n",
      "Stopping the training, the universe has no more patience for this training.\n",
      "Test accuracy = 0.788\n",
      "GAT training: time elapsed= 0.01 [s] | epoch=1 | val acc=0.098\n",
      "GAT training: time elapsed= 1.33 [s] | epoch=101 | val acc=0.736\n",
      "GAT training: time elapsed= 2.67 [s] | epoch=201 | val acc=0.71\n",
      "GAT training: time elapsed= 4.02 [s] | epoch=301 | val acc=0.728\n",
      "GAT training: time elapsed= 5.33 [s] | epoch=401 | val acc=0.758\n",
      "GAT training: time elapsed= 6.69 [s] | epoch=501 | val acc=0.78\n",
      "GAT training: time elapsed= 8.05 [s] | epoch=601 | val acc=0.774\n",
      "GAT training: time elapsed= 9.30 [s] | epoch=701 | val acc=0.768\n",
      "GAT training: time elapsed= 10.59 [s] | epoch=801 | val acc=0.762\n",
      "GAT training: time elapsed= 11.90 [s] | epoch=901 | val acc=0.766\n",
      "GAT training: time elapsed= 13.22 [s] | epoch=1001 | val acc=0.756\n",
      "GAT training: time elapsed= 14.54 [s] | epoch=1101 | val acc=0.752\n",
      "GAT training: time elapsed= 15.86 [s] | epoch=1201 | val acc=0.762\n",
      "GAT training: time elapsed= 17.18 [s] | epoch=1301 | val acc=0.748\n",
      "GAT training: time elapsed= 18.52 [s] | epoch=1401 | val acc=0.754\n",
      "GAT training: time elapsed= 19.90 [s] | epoch=1501 | val acc=0.744\n",
      "Stopping the training, the universe has no more patience for this training.\n",
      "Test accuracy = 0.758\n",
      "GAT training: time elapsed= 0.04 [s] | epoch=1 | val acc=0.102\n",
      "GAT training: time elapsed= 1.46 [s] | epoch=101 | val acc=0.786\n",
      "GAT training: time elapsed= 2.88 [s] | epoch=201 | val acc=0.786\n",
      "GAT training: time elapsed= 4.32 [s] | epoch=301 | val acc=0.776\n",
      "GAT training: time elapsed= 5.73 [s] | epoch=401 | val acc=0.778\n",
      "GAT training: time elapsed= 7.16 [s] | epoch=501 | val acc=0.78\n",
      "GAT training: time elapsed= 8.58 [s] | epoch=601 | val acc=0.782\n",
      "GAT training: time elapsed= 10.01 [s] | epoch=701 | val acc=0.776\n",
      "GAT training: time elapsed= 11.44 [s] | epoch=801 | val acc=0.774\n",
      "GAT training: time elapsed= 12.85 [s] | epoch=901 | val acc=0.776\n",
      "GAT training: time elapsed= 14.25 [s] | epoch=1001 | val acc=0.764\n",
      "GAT training: time elapsed= 15.60 [s] | epoch=1101 | val acc=0.764\n",
      "GAT training: time elapsed= 16.96 [s] | epoch=1201 | val acc=0.75\n",
      "GAT training: time elapsed= 18.32 [s] | epoch=1301 | val acc=0.77\n",
      "Stopping the training, the universe has no more patience for this training.\n",
      "Test accuracy = 0.778\n",
      "GAT training: time elapsed= 0.01 [s] | epoch=1 | val acc=0.138\n",
      "GAT training: time elapsed= 1.38 [s] | epoch=101 | val acc=0.746\n",
      "GAT training: time elapsed= 2.75 [s] | epoch=201 | val acc=0.744\n",
      "GAT training: time elapsed= 4.14 [s] | epoch=301 | val acc=0.748\n",
      "GAT training: time elapsed= 5.45 [s] | epoch=401 | val acc=0.762\n",
      "GAT training: time elapsed= 6.80 [s] | epoch=501 | val acc=0.772\n",
      "GAT training: time elapsed= 8.18 [s] | epoch=601 | val acc=0.756\n",
      "GAT training: time elapsed= 9.55 [s] | epoch=701 | val acc=0.762\n",
      "GAT training: time elapsed= 10.87 [s] | epoch=801 | val acc=0.768\n",
      "GAT training: time elapsed= 12.23 [s] | epoch=901 | val acc=0.764\n",
      "GAT training: time elapsed= 13.51 [s] | epoch=1001 | val acc=0.774\n",
      "GAT training: time elapsed= 14.82 [s] | epoch=1101 | val acc=0.772\n",
      "GAT training: time elapsed= 16.13 [s] | epoch=1201 | val acc=0.772\n",
      "GAT training: time elapsed= 17.45 [s] | epoch=1301 | val acc=0.768\n",
      "GAT training: time elapsed= 18.87 [s] | epoch=1401 | val acc=0.756\n",
      "Stopping the training, the universe has no more patience for this training.\n",
      "Test accuracy = 0.792\n",
      "GAT training: time elapsed= 0.01 [s] | epoch=1 | val acc=0.276\n",
      "GAT training: time elapsed= 1.38 [s] | epoch=101 | val acc=0.762\n",
      "GAT training: time elapsed= 2.82 [s] | epoch=201 | val acc=0.75\n",
      "GAT training: time elapsed= 4.19 [s] | epoch=301 | val acc=0.764\n",
      "GAT training: time elapsed= 5.57 [s] | epoch=401 | val acc=0.784\n",
      "GAT training: time elapsed= 6.94 [s] | epoch=501 | val acc=0.758\n",
      "GAT training: time elapsed= 8.18 [s] | epoch=601 | val acc=0.76\n",
      "GAT training: time elapsed= 9.53 [s] | epoch=701 | val acc=0.748\n",
      "GAT training: time elapsed= 10.72 [s] | epoch=801 | val acc=0.752\n",
      "GAT training: time elapsed= 12.11 [s] | epoch=901 | val acc=0.754\n",
      "GAT training: time elapsed= 13.52 [s] | epoch=1001 | val acc=0.762\n",
      "GAT training: time elapsed= 14.92 [s] | epoch=1101 | val acc=0.754\n",
      "GAT training: time elapsed= 16.32 [s] | epoch=1201 | val acc=0.754\n",
      "GAT training: time elapsed= 17.75 [s] | epoch=1301 | val acc=0.756\n",
      "Stopping the training, the universe has no more patience for this training.\n",
      "Test accuracy = 0.788\n",
      "GAT training: time elapsed= 0.02 [s] | epoch=1 | val acc=0.196\n",
      "GAT training: time elapsed= 1.71 [s] | epoch=101 | val acc=0.158\n",
      "GAT training: time elapsed= 3.39 [s] | epoch=201 | val acc=0.126\n",
      "GAT training: time elapsed= 5.04 [s] | epoch=301 | val acc=0.11\n",
      "GAT training: time elapsed= 6.70 [s] | epoch=401 | val acc=0.122\n",
      "GAT training: time elapsed= 8.38 [s] | epoch=501 | val acc=0.12\n",
      "GAT training: time elapsed= 10.07 [s] | epoch=601 | val acc=0.094\n",
      "GAT training: time elapsed= 11.75 [s] | epoch=701 | val acc=0.178\n",
      "GAT training: time elapsed= 13.43 [s] | epoch=801 | val acc=0.174\n",
      "GAT training: time elapsed= 15.10 [s] | epoch=901 | val acc=0.162\n",
      "GAT training: time elapsed= 16.81 [s] | epoch=1001 | val acc=0.17\n",
      "Stopping the training, the universe has no more patience for this training.\n",
      "Test accuracy = 0.181\n",
      "GAT training: time elapsed= 0.02 [s] | epoch=1 | val acc=0.062\n",
      "GAT training: time elapsed= 1.85 [s] | epoch=101 | val acc=0.162\n",
      "GAT training: time elapsed= 3.82 [s] | epoch=201 | val acc=0.162\n",
      "GAT training: time elapsed= 5.55 [s] | epoch=301 | val acc=0.158\n",
      "GAT training: time elapsed= 7.23 [s] | epoch=401 | val acc=0.156\n",
      "GAT training: time elapsed= 9.04 [s] | epoch=501 | val acc=0.158\n",
      "GAT training: time elapsed= 11.04 [s] | epoch=601 | val acc=0.16\n",
      "GAT training: time elapsed= 13.04 [s] | epoch=701 | val acc=0.156\n",
      "GAT training: time elapsed= 15.01 [s] | epoch=801 | val acc=0.16\n",
      "GAT training: time elapsed= 16.98 [s] | epoch=901 | val acc=0.172\n",
      "GAT training: time elapsed= 18.91 [s] | epoch=1001 | val acc=0.152\n",
      "Stopping the training, the universe has no more patience for this training.\n",
      "Test accuracy = 0.138\n",
      "GAT training: time elapsed= 0.01 [s] | epoch=1 | val acc=0.084\n",
      "GAT training: time elapsed= 1.90 [s] | epoch=101 | val acc=0.21\n",
      "GAT training: time elapsed= 3.79 [s] | epoch=201 | val acc=0.08\n",
      "GAT training: time elapsed= 5.70 [s] | epoch=301 | val acc=0.102\n",
      "GAT training: time elapsed= 7.63 [s] | epoch=401 | val acc=0.094\n",
      "GAT training: time elapsed= 9.44 [s] | epoch=501 | val acc=0.076\n",
      "GAT training: time elapsed= 11.34 [s] | epoch=601 | val acc=0.074\n",
      "GAT training: time elapsed= 13.22 [s] | epoch=701 | val acc=0.074\n",
      "GAT training: time elapsed= 15.12 [s] | epoch=801 | val acc=0.074\n",
      "GAT training: time elapsed= 16.85 [s] | epoch=901 | val acc=0.082\n",
      "GAT training: time elapsed= 18.54 [s] | epoch=1001 | val acc=0.078\n",
      "Stopping the training, the universe has no more patience for this training.\n",
      "Test accuracy = 0.134\n",
      "GAT training: time elapsed= 0.01 [s] | epoch=1 | val acc=0.158\n",
      "GAT training: time elapsed= 1.84 [s] | epoch=101 | val acc=0.266\n",
      "GAT training: time elapsed= 3.80 [s] | epoch=201 | val acc=0.262\n",
      "GAT training: time elapsed= 5.71 [s] | epoch=301 | val acc=0.282\n",
      "GAT training: time elapsed= 7.64 [s] | epoch=401 | val acc=0.296\n",
      "GAT training: time elapsed= 9.59 [s] | epoch=501 | val acc=0.266\n",
      "GAT training: time elapsed= 11.55 [s] | epoch=601 | val acc=0.304\n",
      "GAT training: time elapsed= 13.48 [s] | epoch=701 | val acc=0.368\n",
      "GAT training: time elapsed= 15.43 [s] | epoch=801 | val acc=0.414\n",
      "GAT training: time elapsed= 17.42 [s] | epoch=901 | val acc=0.396\n",
      "GAT training: time elapsed= 19.36 [s] | epoch=1001 | val acc=0.332\n",
      "GAT training: time elapsed= 21.33 [s] | epoch=1101 | val acc=0.356\n",
      "GAT training: time elapsed= 23.26 [s] | epoch=1201 | val acc=0.336\n",
      "GAT training: time elapsed= 25.27 [s] | epoch=1301 | val acc=0.322\n",
      "GAT training: time elapsed= 27.31 [s] | epoch=1401 | val acc=0.34\n",
      "GAT training: time elapsed= 29.31 [s] | epoch=1501 | val acc=0.374\n",
      "GAT training: time elapsed= 31.27 [s] | epoch=1601 | val acc=0.39\n",
      "GAT training: time elapsed= 33.23 [s] | epoch=1701 | val acc=0.398\n",
      "Stopping the training, the universe has no more patience for this training.\n",
      "Test accuracy = 0.425\n",
      "GAT training: time elapsed= 0.02 [s] | epoch=1 | val acc=0.186\n",
      "GAT training: time elapsed= 2.13 [s] | epoch=101 | val acc=0.116\n",
      "GAT training: time elapsed= 4.19 [s] | epoch=201 | val acc=0.12\n",
      "GAT training: time elapsed= 6.27 [s] | epoch=301 | val acc=0.166\n",
      "GAT training: time elapsed= 8.29 [s] | epoch=401 | val acc=0.106\n",
      "GAT training: time elapsed= 10.29 [s] | epoch=501 | val acc=0.13\n",
      "GAT training: time elapsed= 12.33 [s] | epoch=601 | val acc=0.068\n",
      "GAT training: time elapsed= 14.40 [s] | epoch=701 | val acc=0.062\n",
      "GAT training: time elapsed= 16.47 [s] | epoch=801 | val acc=0.102\n",
      "GAT training: time elapsed= 18.52 [s] | epoch=901 | val acc=0.11\n",
      "GAT training: time elapsed= 20.58 [s] | epoch=1001 | val acc=0.106\n",
      "GAT training: time elapsed= 22.68 [s] | epoch=1101 | val acc=0.11\n",
      "GAT training: time elapsed= 24.76 [s] | epoch=1201 | val acc=0.106\n",
      "Stopping the training, the universe has no more patience for this training.\n",
      "Test accuracy = 0.103\n",
      "GAT training: time elapsed= 0.02 [s] | epoch=1 | val acc=0.238\n",
      "GAT training: time elapsed= 2.26 [s] | epoch=101 | val acc=0.102\n",
      "GAT training: time elapsed= 4.42 [s] | epoch=201 | val acc=0.16\n",
      "GAT training: time elapsed= 6.63 [s] | epoch=301 | val acc=0.268\n",
      "GAT training: time elapsed= 8.90 [s] | epoch=401 | val acc=0.382\n",
      "GAT training: time elapsed= 11.12 [s] | epoch=501 | val acc=0.502\n",
      "GAT training: time elapsed= 13.26 [s] | epoch=601 | val acc=0.514\n",
      "GAT training: time elapsed= 15.46 [s] | epoch=701 | val acc=0.474\n",
      "GAT training: time elapsed= 17.39 [s] | epoch=801 | val acc=0.474\n",
      "GAT training: time elapsed= 19.36 [s] | epoch=901 | val acc=0.44\n",
      "GAT training: time elapsed= 21.63 [s] | epoch=1001 | val acc=0.596\n",
      "GAT training: time elapsed= 23.81 [s] | epoch=1101 | val acc=0.682\n",
      "GAT training: time elapsed= 26.02 [s] | epoch=1201 | val acc=0.628\n",
      "GAT training: time elapsed= 28.16 [s] | epoch=1301 | val acc=0.618\n",
      "GAT training: time elapsed= 30.35 [s] | epoch=1401 | val acc=0.666\n",
      "GAT training: time elapsed= 32.54 [s] | epoch=1501 | val acc=0.652\n",
      "GAT training: time elapsed= 34.76 [s] | epoch=1601 | val acc=0.632\n",
      "GAT training: time elapsed= 36.91 [s] | epoch=1701 | val acc=0.652\n",
      "GAT training: time elapsed= 39.12 [s] | epoch=1801 | val acc=0.59\n",
      "GAT training: time elapsed= 41.35 [s] | epoch=1901 | val acc=0.326\n",
      "GAT training: time elapsed= 43.57 [s] | epoch=2001 | val acc=0.374\n",
      "Stopping the training, the universe has no more patience for this training.\n",
      "Test accuracy = 0.437\n",
      "GAT training: time elapsed= 0.02 [s] | epoch=1 | val acc=0.202\n",
      "GAT training: time elapsed= 2.20 [s] | epoch=101 | val acc=0.366\n",
      "GAT training: time elapsed= 4.36 [s] | epoch=201 | val acc=0.688\n",
      "GAT training: time elapsed= 6.51 [s] | epoch=301 | val acc=0.686\n",
      "GAT training: time elapsed= 8.62 [s] | epoch=401 | val acc=0.712\n",
      "GAT training: time elapsed= 10.75 [s] | epoch=501 | val acc=0.688\n",
      "GAT training: time elapsed= 12.94 [s] | epoch=601 | val acc=0.626\n",
      "GAT training: time elapsed= 15.13 [s] | epoch=701 | val acc=0.63\n",
      "GAT training: time elapsed= 17.29 [s] | epoch=801 | val acc=0.676\n",
      "GAT training: time elapsed= 19.48 [s] | epoch=901 | val acc=0.53\n",
      "GAT training: time elapsed= 21.59 [s] | epoch=1001 | val acc=0.464\n",
      "GAT training: time elapsed= 23.73 [s] | epoch=1101 | val acc=0.676\n",
      "GAT training: time elapsed= 25.94 [s] | epoch=1201 | val acc=0.678\n",
      "Stopping the training, the universe has no more patience for this training.\n",
      "Test accuracy = 0.687\n",
      "GAT training: time elapsed= 0.02 [s] | epoch=1 | val acc=0.12\n",
      "GAT training: time elapsed= 2.25 [s] | epoch=101 | val acc=0.206\n",
      "GAT training: time elapsed= 4.56 [s] | epoch=201 | val acc=0.224\n",
      "GAT training: time elapsed= 6.84 [s] | epoch=301 | val acc=0.364\n",
      "GAT training: time elapsed= 9.05 [s] | epoch=401 | val acc=0.576\n",
      "GAT training: time elapsed= 11.28 [s] | epoch=501 | val acc=0.698\n",
      "GAT training: time elapsed= 13.51 [s] | epoch=601 | val acc=0.746\n",
      "GAT training: time elapsed= 15.74 [s] | epoch=701 | val acc=0.742\n",
      "GAT training: time elapsed= 18.03 [s] | epoch=801 | val acc=0.774\n",
      "GAT training: time elapsed= 20.19 [s] | epoch=901 | val acc=0.778\n",
      "GAT training: time elapsed= 22.46 [s] | epoch=1001 | val acc=0.76\n",
      "GAT training: time elapsed= 24.70 [s] | epoch=1101 | val acc=0.746\n",
      "GAT training: time elapsed= 26.91 [s] | epoch=1201 | val acc=0.758\n",
      "GAT training: time elapsed= 29.15 [s] | epoch=1301 | val acc=0.74\n",
      "GAT training: time elapsed= 31.39 [s] | epoch=1401 | val acc=0.74\n",
      "GAT training: time elapsed= 33.67 [s] | epoch=1501 | val acc=0.732\n",
      "GAT training: time elapsed= 35.89 [s] | epoch=1601 | val acc=0.74\n",
      "GAT training: time elapsed= 38.19 [s] | epoch=1701 | val acc=0.742\n",
      "GAT training: time elapsed= 40.37 [s] | epoch=1801 | val acc=0.742\n",
      "GAT training: time elapsed= 42.52 [s] | epoch=1901 | val acc=0.698\n",
      "Stopping the training, the universe has no more patience for this training.\n",
      "Test accuracy = 0.759\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "#\n",
    "# Cora specific constants\n",
    "#\n",
    "\n",
    "# Thomas Kipf et al. first used this split in GCN paper and later Petar Veličković et al. in GAT paper\n",
    "CORA_TRAIN_RANGE = [0, 140]  # we're using the first 140 nodes as the training nodes\n",
    "CORA_VAL_RANGE = [140, 140+500]\n",
    "CORA_TEST_RANGE = [1708, 1708+1000]\n",
    "CORA_NUM_INPUT_FEATURES = 1433\n",
    "CORA_NUM_CLASSES = 7\n",
    "\n",
    "def save_to_file(filename, content):\n",
    "    with open(filename, 'a') as f:\n",
    "        f.write(content)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    filename = f'{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}_cora_test.txt'\n",
    "    save_to_file(filename, 'Cora\\n')\n",
    "    dataset = Planetoid(root='./data/Cora', name='Cora')\n",
    "    time_start = time.time()\n",
    "    for i in range(4):\n",
    "        cases = [[False, False, \"GAT\"], [True, False, \"GAT with Random walk with restart\"], [False, True, \"GAT with Initial residual connection\"], [True, True, \"GAT with Random walk with restart and Initial residual connection\"]]\n",
    "        for case in cases:\n",
    "            content = f\"{i+2} layers {case[-1]}\\n\"\n",
    "            save_to_file(filename, content)\n",
    "            time_start = time.time()\n",
    "            content = train_gat(get_training_args(time_start, dataset, CORA_TRAIN_RANGE, CORA_VAL_RANGE, CORA_TEST_RANGE, CORA_NUM_INPUT_FEATURES, CORA_NUM_CLASSES, random_walk_with_restart=case[0], add_residual_connection=case[1], num_of_additional_layer=i), save_to_file, filename)\n",
    "            save_to_file(filename, f'Total training time: {(time.time() - time_start):.2f} [s]\\n')\n",
    "\n",
    "    save_to_file(filename, f'\\n\\nTotal training time for Full Process: {(time.time() - time_start):.2f} [s]\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citeseer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pubmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pubmed\n",
      "is here?\n",
      "is here?\n",
      "is here?\n",
      "GAT training: time elapsed= 0.03 [s] | epoch=1 | val acc=0.27\n",
      "GAT training: time elapsed= 1.81 [s] | epoch=101 | val acc=0.766\n",
      "GAT training: time elapsed= 3.66 [s] | epoch=201 | val acc=0.76\n",
      "GAT training: time elapsed= 5.46 [s] | epoch=301 | val acc=0.726\n",
      "GAT training: time elapsed= 7.19 [s] | epoch=401 | val acc=0.786\n",
      "GAT training: time elapsed= 9.04 [s] | epoch=501 | val acc=0.774\n",
      "GAT training: time elapsed= 10.76 [s] | epoch=601 | val acc=0.766\n",
      "GAT training: time elapsed= 12.66 [s] | epoch=701 | val acc=0.752\n",
      "GAT training: time elapsed= 14.53 [s] | epoch=801 | val acc=0.756\n",
      "GAT training: time elapsed= 16.42 [s] | epoch=901 | val acc=0.732\n",
      "GAT training: time elapsed= 18.27 [s] | epoch=1001 | val acc=0.766\n",
      "GAT training: time elapsed= 20.13 [s] | epoch=1101 | val acc=0.762\n",
      "Stopping the training, the universe has no more patience for this training.\n",
      "Test accuracy = 0.76\n",
      "Total training time: 20.75 [s]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "#\n",
    "# Pubmed specific constants\n",
    "#\n",
    "\n",
    "# Thomas Kipf et al. first used this split in GCN paper and later Petar Veličković et al. in GAT paper\n",
    "PUBMED_TRAIN_RANGE = [0, 60]  # we're using the first 140 nodes as the training nodes\n",
    "PUBMED_VAL_RANGE = [60, 60+500]\n",
    "PUBMED_TEST_RANGE = [18717, 18717+1000]\n",
    "PUBMED_NUM_INPUT_FEATURES = 500\n",
    "PUBMED_NUM_CLASSES = 3\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print('Pubmed')\n",
    "    dataset = Planetoid(root='./data/Pubmed', name='Pubmed')\n",
    "    time_start = time.time()\n",
    "    train_gat(get_training_args(time_start, dataset, PUBMED_TRAIN_RANGE, PUBMED_VAL_RANGE, PUBMED_TEST_RANGE, PUBMED_NUM_INPUT_FEATURES, PUBMED_NUM_CLASSES))\n",
    "    # train_gat(time_start, dataset, PUBMED_TRAIN_RANGE, PUBMED_VAL_RANGE, PUBMED_TEST_RANGE, PUBMED_NUM_INPUT_FEATURES, PUBMED_NUM_CLASSES)\n",
    "    print(f'Total training time: {(time.time() - time_start):.2f} [s]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
